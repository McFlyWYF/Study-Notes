### 神经网络风格迁移

![1585802462984](C:\Users\16500\AppData\Local\Temp\1585802462984.png)

![1585803847862](C:\Users\16500\AppData\Local\Temp\1585803847862.png)

![1585813172834](C:\Users\16500\AppData\Local\Temp\1585813172834.png)

5张图分别来自VGG的`conv1 1 (a)`, `conv2 1 (b)`, `conv3 1 (c)`, `conv4 1 (d)` ,`conv5 1(e)`。 

##### 微调（Fine-tune）原理

* 在自己的数据集上训练一个新的深度学习模型时，一般采取在预训练好的模型上进行微调的方法。
* VGGNet16

![1585804038942](C:\Users\16500\AppData\Local\Temp\1585804038942.png)

![1585804132398](C:\Users\16500\AppData\Local\Temp\1585804132398.png)

* VGG16的结构为卷积+全连接 = 16层。卷积层分为5个部分共13层，即图中的conv1~conv5。还有3层是全连接层，即图中的fc6、fc7、fc8。如果要将VGG16的结构用于一个新的数据集，首先要去掉fc8这一层。原因是fc8层的输入是fc7的特征，输出是1000类的概率，这1000类正好对应ImageNet模型中的1000个类别。采用符合数据集类别数的全连接层作为新的fc8。
* 在训练的时候，网络参数的初始值采用VGG16在ImageNet上已经训练好的参数作为训练的初始值。
* 载入VGG16的参数后，可以开始训练了。一般来说，可以选择以下几种范围进行训练：
  * 只训练fc8，好处是训练速度快，但往往性能不会太好。
  * 训练所有参数。训练速度慢，但是能取得较高的性能，可以充分发挥深度模型的威力。
  * 训练部分参数。通常是固定浅层参数不变，训练深层参数。如固定coonv1、conv2部分的参数不训练，只训练conv3、conv4、conv5、fc6、fc7、fc8的参数。
* 微调的原理大致就是先看懂网络的结构图，然后把网络的一部分修改成子集需要的模型。这种训练方法就是所谓的对神经网络模型做微调。借助微调，可以从预训练模型出发，将神经网络应用到自己的数据集上。

#### 原始图像风格迁移

* VGGNet是输入图像，提取特征，并输出图像类别。图像风格迁移正好与其相反，输入的是特征，输出对应这种特征的图片。

  ![1585805170085](C:\Users\16500\AppData\Local\Temp\1585805170085.png)

*     还原图像的方法是梯度下降法。

      ![1585805306501](C:\Users\16500\AppData\Local\Temp\1585805306501.png)

      ![1585805324951](C:\Users\16500\AppData\Local\Temp\1585805324951.png)

*     还原图像的风格的方法是使用图像的卷积层特征的Gram矩阵。

      * Gram矩阵：是为了表达图像的纹理特征。Gram就是对两个feature map求内积，结果和位置没有关系。
        * 在feature map中，每个数字都来自于一个特点滤波器在特定位置的卷积，因此每个数字代表一个特征的强度，而Gram计算的实际是两两特征之间的相关性，哪两个特征是同时出现的，哪两个是此消彼长的。
        * 同时，Gram的对角线元素，体现了每个特征在图像中出现的量。
        * 有助于把握整个图像的大体风格。

      ![1585805424637](C:\Users\16500\AppData\Local\Temp\1585805424637.png)

      ![1585805451887](C:\Users\16500\AppData\Local\Temp\1585805451887.png)

      ![1585805520417](C:\Users\16500\AppData\Local\Temp\1585805520417.png)

*     **利用内容损失还原图像内容。**

*     **利用风格损失还原图像风格。**

*     将内容损失和风格损失结合起来，在还原图像内容的同时还原图像风格。

      ![1585805708899](C:\Users\16500\AppData\Local\Temp\1585805708899.png)

*     缺点：

      ![1585805912469](C:\Users\16500\AppData\Local\Temp\1585805912469.png)

#### 快速图像风格迁移原理

* 快速图像风格迁移的方法是：不使用优化的方法来逐步迭代生成x，而是使用一个神经网络生成x。网络结构如图：

  ![1585806034623](C:\Users\16500\AppData\Local\Temp\1585806034623.png)

  * 整个系统由两个神经网络组成，他们在图中由两个休闲裤分别标出。左边是图像生成网络，右边是损失网络。损失网络实际是VGGNet。利用损失网络来定义内容损失、风格损失。这个损失用来实例图像生成网络。图像生成网络的职责是生成某一种风格的图像，它的输入是一张图像，输出同样是一张图像。由于输出图像只需要在网络中计算一遍，所以速度快。

  ![1585806271738](C:\Users\16500\AppData\Local\Temp\1585806271738.png)

* 原始图像风格迁移与快速图像风格迁移比较

  ![1585806445150](C:\Users\16500\AppData\Local\Temp\1585806445150.png)

  

![1585813619967](C:\Users\16500\AppData\Local\Temp\1585813619967.png)

* 当在网络的低层上匹配内容，算法会匹配照片上的大部分像素细节信息，生成的图像似乎艺术图的纹理几乎不融合进照片中。相反，在网络高层上匹配内容特征，照片的像素细节信息没有很强的约束，艺术画的纹理和照片的内容恰当地融合在一起。也就是说，图像中明确的结构，比如边缘和颜色地图会被改变，使用艺术画的风格和照片的内容。